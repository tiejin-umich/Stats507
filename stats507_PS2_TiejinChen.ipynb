{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784cfd6f",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "### Problem 0\n",
    "We have the following code to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8)]\n",
    "op = []\n",
    "for m in range(len(sample_list)):\n",
    "    li = [sample_list[m]]\n",
    "        for n in range(len(sample_list)):\n",
    "            if (sample_list[m][0] == sample_list[n][0] and\n",
    "                    sample_list[m][3] != sample_list[n][3]):\n",
    "                li.append(sample_list[n])\n",
    "        op.append(sorted(li, key=lambda dd: dd[3], reverse=True)[0])\n",
    "res = list(set(op))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff257df",
   "metadata": {},
   "source": [
    "Purpose: The snippet takes a list of tuples and, after a small correction, returns a list of tuples having the maximum 3rd element among tuples sharing a common first element.\n",
    "\n",
    "We give the following code review:\n",
    "1. The code needs to work. author needs to focos on where and when to indent.\n",
    "Also, keep in mind that the index of list in Python starts at 0.\n",
    "Hence we need to use 2 to replace 3 in code.\n",
    "2. The code is very easy to read and Pythonic. For example,\n",
    "the author uses an anonymous function when using sorted function which is good.\n",
    "3. Iterate over indices only when necessary, else iterate over values.\n",
    "It can make the code run more quickly and author might needs to improve\n",
    "4. The style author write is not bad. It keeps every row with reasonable characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39b1f7",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Before we start, we first import all the packages we needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3095f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c08f6",
   "metadata": {},
   "source": [
    "Now, we define a python function ```random_tuple``` to create a list with random tuple with 4 parameters.\n",
    "And in the end, we use assert to confirm the every element in the list is a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a2f4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all element are tuple\n"
     ]
    }
   ],
   "source": [
    "def random_tuple(n,k=5,low=0,high=9):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n: type int;\n",
    "    present the number of random tuple we create\n",
    "    :param k: type int:\n",
    "    present the number of random int in every tuple we create\n",
    "    :param low: number type(eg. int or float);\n",
    "    present the lowest bound the range where we choose\n",
    "    :param high:\n",
    "    number type(eg. int or float);\n",
    "    present the upper bound the range where we choose\n",
    "    :return: type list;\n",
    "    :return a list with every list[i] is a tuple with random int value bewteen low to high.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    if n < 0 or k < 0:\n",
    "        raise ValueError('n and k must be greater than 0')\n",
    "    if type(n) != type(1) or type(k) != type(1):\n",
    "        raise TypeError('n and k must be int')\n",
    "    if type(low) == type(0.5):\n",
    "        low = int(low) +1\n",
    "    elif type(low) != type(1):\n",
    "        raise TypeError('low must be a number')\n",
    "    if type(high) == type(0.5):\n",
    "        high = int(high)\n",
    "    elif type(high) != type(1):\n",
    "        raise TypeError('high must be a number')\n",
    "    for i in range(n):\n",
    "        ran_tup = tuple(np.random.choice(range(low,high+1),k))\n",
    "        res.append(ran_tup)\n",
    "    return res\n",
    "\n",
    "test = random_tuple(10)\n",
    "for tup in test:\n",
    "    assert type(tup) == tuple\n",
    "print(\"all element are tuple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d863ea6",
   "metadata": {},
   "source": [
    "### Problem2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2e957",
   "metadata": {},
   "source": [
    "#### Part a\n",
    "We define a function ```class_max_by_key ``` which means gets the max tuple by one position\n",
    "from different class to encapsulate the code. Class here can also been seen as a group_by.\n",
    "That is to say, we \"group_by\" the tuple using class, and then we take every tuple\n",
    "which has the max value of one column from each group to form a list. Hence besides the data itself,\n",
    "we need 2 more parameters. One is ```class_key``` that can been seen as a position which we use\n",
    "class_key-th column to group_bp. And the other one is ```sort_key``` that can also been seen as\n",
    "a position which we use sort_key-th column to decide which tuple is the max.\n",
    "\n",
    "It is easy to see that this two parameters should not been the same.\n",
    "Or we can just get the input data without repeated tuple.\n",
    "And we use 3 sample list to show the accuracy of our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb9aca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2), (1, 9, 8)]\n",
      "[(1, 3, 5), (0, 1, 2), (2, 1, 3)]\n",
      "[(1, 3, 5), (2, 0, 7), (0, 1, 2), (1, 9, 5)]\n"
     ]
    }
   ],
   "source": [
    "def class_max_by_key(list_data,class_key,sort_key):\n",
    "    \"\"\"\n",
    "\n",
    "    :param list_data: type list;\n",
    "    the data we want to handle\n",
    "    :param class_key: type int;\n",
    "    present the position key that class we have.\n",
    "    :param sort_key: type int;\n",
    "    represent which position key to sort.\n",
    "    Here we assumed sort_key cannot equal to class_key\n",
    "    :return: type list;\n",
    "    we will return a list containing the max tuple according to the sort_key from different class.\n",
    "    \"\"\"\n",
    "    if class_key == sort_key:\n",
    "        warnings.warn(\"class_key should not equal to sort_key\")\n",
    "        return list(set(list_data))\n",
    "    op = []\n",
    "    for m in range(len(list_data)):\n",
    "        li = [list_data[m]]\n",
    "        for n in range(len(list_data)):\n",
    "            if (list_data[m][class_key] == list_data[n][class_key] and\n",
    "                    list_data[m][sort_key] != list_data[n][sort_key]):\n",
    "                li.append(list_data[n])\n",
    "        op.append(sorted(li, key=lambda dd: dd[sort_key], reverse=True)[0])\n",
    "    res = list(set(op))\n",
    "    return res\n",
    "\n",
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8)]\n",
    "print(class_max_by_key(sample_list,0,2))\n",
    "\n",
    "sample_list2 = [(1, 3, 5), (0, 1, 2), (1, 9, 4),(2, 1, 3),(0,6,1)]\n",
    "print(class_max_by_key(sample_list2,0,2))\n",
    "\n",
    "sample_list3 = [(1, 3, 5), (0, 1, 2), (1, 9, 5),(2, 1, 3),(0,6,1),(2,0,7)]\n",
    "print(class_max_by_key(sample_list3,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62705092",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "With the code review we made, we write a function ```class_max_by_key_improved``` to\n",
    "improve the code. We use same 3 sample list to show the accuracy of fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d4266e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2), (1, 9, 8)]\n",
      "[(1, 3, 5), (0, 1, 2), (2, 1, 3)]\n",
      "[(1, 3, 5), (2, 0, 7), (0, 1, 2), (1, 9, 5)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "def class_max_by_key_improved(list_data,class_key,sort_key):\n",
    "    \"\"\"\n",
    "\n",
    "    :param list_data: type list;\n",
    "    the data we want to handle\n",
    "    :param class_key: type int;\n",
    "    present the position key that class we have.\n",
    "    :param sort_key: type int;\n",
    "    represent which position key to sort.\n",
    "    Here we assumed sort_key cannot equal to class_key\n",
    "    :return: type list;\n",
    "    we will return a list containing the max tuple according to the sort_key from different class.\n",
    "    \"\"\"\n",
    "    if class_key == sort_key:\n",
    "        warnings.warn(\"class_key should not equal to sort_key\")\n",
    "        return list(set(list_data))\n",
    "    op = []\n",
    "    for tup in list_data:\n",
    "        temp_li = [tup]\n",
    "        for tup_2 in list_data:\n",
    "            if (tup[class_key] == tup_2[class_key] and\n",
    "                    tup[sort_key] != tup_2[sort_key]):\n",
    "                temp_li.append(tup_2)\n",
    "        op.append(sorted(temp_li, key=lambda dd: dd[sort_key], reverse=True)[0])\n",
    "    return list(set(op))\n",
    "\n",
    "print(class_max_by_key_improved(sample_list,0,2))\n",
    "print(class_max_by_key_improved(sample_list2,0,2))\n",
    "print(class_max_by_key_improved(sample_list3,0,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa3f46",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "We write a function ```class_max_by_key_dict``` to improve the code further with a new core\n",
    "algorithm. Using ```dict``` , the built-in Python data structure with the idea of Hashmap,\n",
    "we reduce the time complexity of original code $O(n^2)$  to  $O(n)$.\n",
    "We use same 3 sample list to show the accuracy of fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cfe3bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 9, 8), (0, 1, 2)]\n",
      "[(1, 3, 5), (0, 1, 2), (2, 1, 3)]\n",
      "[(1, 3, 5), (1, 9, 5), (0, 1, 2), (2, 0, 7)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "def class_max_by_key_dict(list_data,class_key,sort_key):\n",
    "    \"\"\"\n",
    "\n",
    "    :param list_data: type list;\n",
    "    the data we want to handle\n",
    "    :param class_key: type int;\n",
    "    present the position key that class we have.\n",
    "    :param sort_key: type int;\n",
    "    represent which position key to sort.\n",
    "    Here we assumed sort_key cannot equal to class_key\n",
    "    :return: type list;\n",
    "    we will return a list containing the max tuple according to the sort_key from different class.\n",
    "    \"\"\"\n",
    "    if class_key == sort_key:\n",
    "        warnings.warn(\"class_key should not equal to sort_key\")\n",
    "        return list(set(list_data))\n",
    "    max_dict = {}\n",
    "    res_dict = {}\n",
    "    for tup in list_data:\n",
    "        if tup[class_key] not in max_dict.keys() or \\\n",
    "                tup[sort_key] > max_dict[tup[class_key]]:\n",
    "            max_dict[tup[class_key]] = tup[sort_key]\n",
    "            res_dict[tup[class_key]] = [tup]\n",
    "            continue\n",
    "        if tup[sort_key] == max_dict[tup[class_key]]:\n",
    "            res_dict[tup[class_key]].append(tup)\n",
    "    res = []\n",
    "    for res_list in res_dict.values():\n",
    "        res += res_list\n",
    "    return res\n",
    "\n",
    "print(class_max_by_key_dict(sample_list,0,2))\n",
    "print(class_max_by_key_dict(sample_list2,0,2))\n",
    "print(class_max_by_key_dict(sample_list3,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24863637",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Part d\n",
    "We write a function ```time_estimate ``` to get a nice table with the result of\n",
    "our Monte Carlo experiments.\n",
    "In total, we will raise our n which is the number of tuple in one list from 5 to 1000.\n",
    "For every n, we will run  experiment 100 times, and we record the mean of 100 time experiments.\n",
    "Finally, in the table, 'standard' presents the code in **Part a**, improved presents the code\n",
    "in **Part b** and dict_method presents the code in **Part c**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cb353d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 5         50        100       200       500       1000\n",
      "standard     0.000008  0.000184  0.000673  0.002583  0.017114  0.070178\n",
      "improved     0.000005  0.000135  0.000502  0.001959  0.012496  0.050889\n",
      "dict_method  0.000002  0.000012  0.000022  0.000043  0.000106  0.000208\n"
     ]
    }
   ],
   "source": [
    "def time_estimate(n_list,num):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_list: type list;\n",
    "    every element presents the number of tuple in one list.\n",
    "    :param num: type int;\n",
    "    for each n, the number of experiment we run.\n",
    "    The total experiments we run will be num*len(n_list)\n",
    "    :return: type pd.DataFrame;\n",
    "    return a dataframe we can show.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for n in n_list:\n",
    "        res_stand = 0\n",
    "        res_improved = 0\n",
    "        res_dict = 0\n",
    "        res_temp = {}\n",
    "        for i in range(num):\n",
    "            list_experiment = random_tuple(n)\n",
    "            time1 = time.perf_counter()\n",
    "            class_max_by_key(list_experiment,0,3)\n",
    "            time2 = time.perf_counter()\n",
    "            class_max_by_key_improved(list_experiment,0,3)\n",
    "            time3 = time.perf_counter()\n",
    "            class_max_by_key_dict(list_experiment,0,3)\n",
    "            time4 = time.perf_counter()\n",
    "            res_stand = (res_stand*i + time2-time1)/(i+1)\n",
    "            res_improved = (res_improved*i + time3-time2)/(i+1)\n",
    "            res_dict = (res_dict*i + time4-time3)/(i+1)\n",
    "        res_temp['standard'] = res_stand\n",
    "        res_temp['improved'] = res_improved\n",
    "        res_temp['dict_method'] = res_dict\n",
    "        res[n] = res_temp\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "print(time_estimate([5,50,100,200,500,1000],100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a683779",
   "metadata": {},
   "source": [
    "It is easy to see that after improved the code, the time cost is decreasing.\n",
    "However they are still in the same order of magnitude. And the code in **Part c** uses\n",
    "much less time than others. We can see it that, when n increase from 50 to 100. n has doubled.\n",
    "And the time cost of standard and improved increase 4 times which is $n^2$ while dict_method only\n",
    "increase 2 times which is $n$. That coincides with the time complexity we said in **Part c**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a10d6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Problem 3\n",
    "#### Part a\n",
    "In the following code, we use ```convert_dtypes()``` method to convert the data which should be\n",
    "int but is float due to the existence of $nan$ into int64. And with this method, we do not need\n",
    "to deal with the $nan$ value. And we do not know if we need to do this, but we also set the Unique ids\n",
    "as the index of all data. We also change the 'cohort' column to categorical data.\n",
    "Finally we use ```to_pickle``` method to save the dataframe as the pickle file.\n",
    "We only retain the columns we need and give them literate names of course.\n",
    "And we do not handle with the categorical data expect cohort.\n",
    "Since we do not know the exact meaning of value for other categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6498e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Age  Race and Ethnicity  Education   Marital Status  \\\n",
      "UniqueID                                                        \n",
      "62161      22                   3          3                5   \n",
      "62162       3                   1       <NA>             <NA>   \n",
      "62163      14                   6       <NA>             <NA>   \n",
      "62164      44                   3          4                1   \n",
      "62165      14                   4       <NA>             <NA>   \n",
      "...       ...                 ...        ...              ...   \n",
      "102952     70                   6          3                1   \n",
      "102953     42                   1          3                4   \n",
      "102954     41                   4          5                5   \n",
      "102955     14                   4       <NA>             <NA>   \n",
      "102956     38                   3          4                3   \n",
      "\n",
      "          Examination Status  Pseudo-PSU  Pseudo-Stratum     MEC weight  \\\n",
      "UniqueID                                                                  \n",
      "62161                      2           1              91  104236.582554   \n",
      "62162                      2           3              92   16116.354010   \n",
      "62163                      2           3              90    7869.485117   \n",
      "62164                      2           1              94  127965.226204   \n",
      "62165                      2           2              90   13384.042162   \n",
      "...                      ...         ...             ...            ...   \n",
      "102952                     2           2             138   18338.711104   \n",
      "102953                     2           2             137   63661.951573   \n",
      "102954                     2           1             144   17694.783346   \n",
      "102955                     2           1             136   14871.839636   \n",
      "102956                     2           1             142   39426.299948   \n",
      "\n",
      "          Interview weight     cohort  \n",
      "UniqueID                               \n",
      "62161        102641.406474  2011-2012  \n",
      "62162         15457.736897  2011-2012  \n",
      "62163          7397.684828  2011-2012  \n",
      "62164        127351.373299  2011-2012  \n",
      "62165         12209.744980  2011-2012  \n",
      "...                    ...        ...  \n",
      "102952        16896.276203  2017-2018  \n",
      "102953        61630.380013  2017-2018  \n",
      "102954        17160.895269  2017-2018  \n",
      "102955        14238.445922  2017-2018  \n",
      "102956        38645.740291  2017-2018  \n",
      "\n",
      "[39156 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "sas_list = [\"Demo_G.XPT\",\"DEMO_H.XPT\",\"DEMO_I.XPT\",\"DEMO_J.XPT\"]\n",
    "res = []\n",
    "for i in range(len(sas_list)):\n",
    "    name_list = ['SEQN','RIDAGEYR','RIDRETH3','DMDEDUC2','DMDMARTL',\n",
    "                 'RIDSTATR','SDMVPSU','SDMVSTRA','WTMEC2YR','WTINT2YR']\n",
    "    df = pd.read_sas(sas_list[i])[name_list].convert_dtypes()\n",
    "    df['RIDAGEYR'] = df['RIDAGEYR'].astype(\"int64\")\n",
    "    df.columns = ['UniqueID','Age','Race and Ethnicity','Education',' Marital Status',\n",
    "                  'Examination Status','Pseudo-PSU','Pseudo-Stratum','MEC weight','Interview weight']\n",
    "    df.set_index('UniqueID',inplace=True)\n",
    "    df['cohort'] = i\n",
    "    res.append(df)\n",
    "res = pd.concat(res)\n",
    "res['cohort'] = pd.Categorical(res['cohort'].replace({0:\"2011-2012\",1:\"2013-2014\",\n",
    "                                                    2:\"2015-2016\",3:\"2017-2018\"}))\n",
    "print(res)\n",
    "res.to_pickle(\"demographic data.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4519d9",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "We do the same thing as **Part a**. However, we do not know the exact characters in OHXxxTC and OHXxxCTC.\n",
    "Hence we need to get all these colmuns first. Here we use regular expression to extract the exact names\n",
    "which colmuns we need to retain from original data. We create the ```name_list``` which\n",
    "is a list to store the colmuns names we want to retain\n",
    "and ```columns_name``` is a list to store the literate name we give first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fed74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_list_part_c = [\"OHXDEN_G.XPT\",\"OHXDEN_H.XPT\",\"OHXDEN_I.XPT\",\"OHXDEN_J.XPT\"]\n",
    "res_c = []\n",
    "name_list = ['SEQN', 'OHDDESTS']\n",
    "columns_name = ['UniqueID','Dentition Status']\n",
    "regu_tooth = r'OHX\\d\\dTC'\n",
    "regu_coro = r'OHX\\d\\dCTC'\n",
    "all_col = pd.read_sas(\"OHXDEN_G.XPT\").columns\n",
    "for col in all_col:\n",
    "    if re.search(regu_tooth,col) != None:\n",
    "        name_list.append(col)\n",
    "        liter_name = \"Tooth_Count_\" + col[3:5]\n",
    "        columns_name.append(liter_name)\n",
    "for col in all_col:\n",
    "    if re.search(regu_coro,col) != None:\n",
    "        name_list.append(col)\n",
    "        liter_name = \"Coronal_Cavities_\" + col[3:5]\n",
    "        columns_name.append(liter_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef78e83",
   "metadata": {},
   "source": [
    "Use the two list to get the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da361594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dentition Status  Tooth_Count_01  Tooth_Count_02  Tooth_Count_03  \\\n",
      "UniqueID                                                                     \n",
      "62161                    1               4               2               2   \n",
      "62162                    1               4               4               4   \n",
      "62163                    1               4               2               2   \n",
      "62164                    1               4               2               2   \n",
      "62165                    1               4               2               2   \n",
      "...                    ...             ...             ...             ...   \n",
      "102952                   1               2               2               2   \n",
      "102953                   1               2               2               2   \n",
      "102954                   1               2               2               2   \n",
      "102955                   1               4               2               2   \n",
      "102956                   1               4               4               2   \n",
      "\n",
      "          Tooth_Count_04  Tooth_Count_05  Tooth_Count_06  Tooth_Count_07  \\\n",
      "UniqueID                                                                   \n",
      "62161                  2               2               2               2   \n",
      "62162                  1               1               1               1   \n",
      "62163                  2               2               2               2   \n",
      "62164                  2               2               2               2   \n",
      "62165                  2               2               2               2   \n",
      "...                  ...             ...             ...             ...   \n",
      "102952                 2               2               2               2   \n",
      "102953                 2               2               2               2   \n",
      "102954                 2               2               2               2   \n",
      "102955                 2               2               2               2   \n",
      "102956                 2               2               2               2   \n",
      "\n",
      "          Tooth_Count_08  Tooth_Count_09  ...  Coronal_Cavities_23  \\\n",
      "UniqueID                                  ...                        \n",
      "62161                  2               2  ...                 b'S'   \n",
      "62162                  1               1  ...                 b'D'   \n",
      "62163                  2               2  ...                 b'S'   \n",
      "62164                  2               2  ...                 b'S'   \n",
      "62165                  2               2  ...                 b'S'   \n",
      "...                  ...             ...  ...                  ...   \n",
      "102952                 2               2  ...                 b'S'   \n",
      "102953                 2               2  ...                 b'S'   \n",
      "102954                 2               2  ...                 b'S'   \n",
      "102955                 2               2  ...                 b'S'   \n",
      "102956                 2               2  ...                 b'S'   \n",
      "\n",
      "          Coronal_Cavities_24  Coronal_Cavities_25  Coronal_Cavities_26  \\\n",
      "UniqueID                                                                  \n",
      "62161                    b'S'                 b'S'                 b'S'   \n",
      "62162                    b'D'                 b'D'                 b'D'   \n",
      "62163                    b'S'                 b'S'                 b'S'   \n",
      "62164                    b'S'                 b'S'                 b'S'   \n",
      "62165                    b'S'                 b'S'                 b'S'   \n",
      "...                       ...                  ...                  ...   \n",
      "102952                   b'S'                 b'S'                 b'S'   \n",
      "102953                   b'S'                 b'S'                 b'S'   \n",
      "102954                   b'S'                 b'S'                 b'S'   \n",
      "102955                   b'S'                 b'S'                 b'S'   \n",
      "102956                   b'S'                 b'S'                 b'S'   \n",
      "\n",
      "          Coronal_Cavities_27  Coronal_Cavities_28  Coronal_Cavities_29  \\\n",
      "UniqueID                                                                  \n",
      "62161                    b'S'                 b'S'                 b'S'   \n",
      "62162                    b'D'                 b'D'                 b'D'   \n",
      "62163                    b'S'                 b'S'                 b'S'   \n",
      "62164                    b'S'                 b'S'                 b'S'   \n",
      "62165                    b'S'                 b'S'                 b'S'   \n",
      "...                       ...                  ...                  ...   \n",
      "102952                   b'S'                 b'S'                 b'S'   \n",
      "102953                   b'S'                 b'S'                 b'S'   \n",
      "102954                   b'S'                 b'S'                 b'F'   \n",
      "102955                   b'S'                 b'S'                 b'S'   \n",
      "102956                   b'S'                 b'S'                 b'S'   \n",
      "\n",
      "          Coronal_Cavities_30  Coronal_Cavities_31     cohort  \n",
      "UniqueID                                                       \n",
      "62161                    b'Z'                 b'S'  2011-2012  \n",
      "62162                    b'U'                 b'U'  2011-2012  \n",
      "62163                    b'Y'                 b'S'  2011-2012  \n",
      "62164                    b'Z'                 b'Z'  2011-2012  \n",
      "62165                    b'S'                 b'S'  2011-2012  \n",
      "...                       ...                  ...        ...  \n",
      "102952                   b'S'                 b'S'  2017-2018  \n",
      "102953                   b'S'                 b'Z'  2017-2018  \n",
      "102954                   b'S'                 b'S'  2017-2018  \n",
      "102955                   b'S'                 b'Z'  2017-2018  \n",
      "102956                   b'S'                 b'E'  2017-2018  \n",
      "\n",
      "[35909 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sas_list_part_c)):\n",
    "    df = pd.read_sas(sas_list_part_c[i])[name_list].convert_dtypes()\n",
    "    df.columns = columns_name\n",
    "    df.set_index('UniqueID',inplace=True)\n",
    "    df['cohort'] = i\n",
    "    res_c.append(df)\n",
    "res_c = pd.concat(res_c)\n",
    "res_c['cohort'] = pd.Categorical(res_c['cohort'].replace({0:\"2011-2012\",1:\"2013-2014\",\n",
    "                                                    2:\"2015-2016\",3:\"2017-2018\"}))\n",
    "print(res_c)\n",
    "res_c.to_pickle(\"oral health dentition data.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a2192",
   "metadata": {},
   "source": [
    "### Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6948fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39156, 10)\n",
      "(35909, 62)\n"
     ]
    }
   ],
   "source": [
    "print(res.shape)\n",
    "print(res_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d3632",
   "metadata": {},
   "source": [
    "We have the case number of **Part a** dataset is 39156 and the case number of **Part b** dataset is 35909"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,auto:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
